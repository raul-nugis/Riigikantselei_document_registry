{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i0.wp.com/kpmgeestiblog.ee/wp-content/uploads/2018/12/cropped-LOGIO_KPMG_NoCP_RGB_280-2.png?resize=110%2C51&ssl=1\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i0.wp.com/kpmgeestiblog.ee/wp-content/uploads/2018/12/cropped-LOGIO_KPMG_NoCP_RGB_280-2.png?resize=110%2C51&ssl=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web-mining document registry\n",
    "\n",
    "\n",
    "#### The purpose of this notebook is to quickly:\n",
    "1. Review Riigikantselei's (Chancellory of Government Office of the Estonia) public Document Registry, \n",
    "2. Understand structure of document entries and define several sample fields (like type of document, direction of document flow, date etc)\n",
    "3. Devise functions to web-scrape contents\n",
    "4. Run a scraper for last full month (May) and save results as structured date to table file\n",
    "6. Check results\n",
    "\n",
    "The time-frame to devise and implement scraper is 1-2 hours\n",
    "\n",
    "link to registry:\n",
    "https://dhs.riigikantselei.ee/avalikteave.nsf/byjournalkey?open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Review Riigikantselei's (Chancellory of Government Office of the Estonia) public Document Registry\n",
    "\n",
    "The registry has comprehensive search engine\n",
    "<br>The registry has got customized html tags like\n",
    "<br><i>&lt;fieldtitle name=\"receivedfrom\">Kellelt saabunud&lt;/fieldtitle></i>\n",
    "<br>The registry uses HEX numbering to generate links to documents\n",
    "<i>\"https://dhs.riigikantselei.ee/avalikteave.nsf/documents/NT003822E6\"</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Understand structure of document entries and define several sample fields\n",
    "\n",
    "The fields can be parsed out by manipulated html text. This is less ideal than using beautiful Soup functionalities, especially to parse out HTML tables. However, Riigikatselei's document registry uses completely customized tags for tables, instead of standard one. Also, there are only certain fields to be taken for this business task, such as receivers, senders, type of document and very few others\n",
    "\n",
    "#### 3. Devise functions to web-scrape contents\n",
    "\n",
    "The fields are defined in a .py module \"riigikantselei_functions\"\n",
    "<br>There are three functions visible below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions defined:\n",
      "\n",
      " get_web_contents(link)\t\t\treturns \"contents\"\n",
      " get_links(soup)\t\t\treturns \"links\"\n",
      " parse_entry(contents, url, counter)\treturns \"dataframe\"\n"
     ]
    }
   ],
   "source": [
    "import riigikantselei_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fields to scrape and place to table format are defined inside function\n",
    "\"parse_entry\" and can be printed from inner help of that funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function parse_entry in module riigikantselei_functions:\n",
      "\n",
      "parse_entry(contents, url, counter)\n",
      "    This function parses out the contents of one document entry in Document Registry\n",
      "    The idea is to use ad-hoc functionalities based on text manipulation and obtain values \n",
      "    for only select fields, which are:\n",
      "                         columns = ['URL',\n",
      "                                'Kellelt',\n",
      "                                'Kellele',\n",
      "                                'Väljaandja',\n",
      "                                'Dok No', \n",
      "                                'Kuupäev',\n",
      "                                'Dok Tüüp', \n",
      "                                'Dok Klass', \n",
      "                                'AK']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from riigikantselei_functions import get_web_contents, get_links, parse_entry\n",
    "\n",
    "help(parse_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Run a scraper for last full month (May) and save results as structured date to table file\n",
    "\n",
    "As mentioned above, Riigikantselei has - in addition to html tags - also customized links by replacing normal decimal incrementation with HEX incrementation. Therefore in the below code I create some 16341 links, which encompass May 2021\n",
    "\n",
    "The initial link is based on patterns of links in the first few days of May, which were found by manual review. 16K additional links generated are very much likely to contain all of the document turnover registered in the registry in May 2021, the whole month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hexes for links\n",
    "\n",
    "selection = [hex(number).split('x')[-1] for number in range(0,256)]\n",
    "selection = ['0' + number.upper() if len(number) == 1 else number.upper() for number in selection]\n",
    "\n",
    "generated_links = []\n",
    "\n",
    "base = 'https://dhs.riigikantselei.ee'\n",
    "iterative_part_numbers = '/avalikteave.nsf/documents/NT'#003822' # two figures are missing\n",
    "\n",
    "for i in range(16341):\n",
    "    generated_number = '00' + str(hex(int('37E2A2', 16) + i)).upper().split('X')[-1]\n",
    "    generated_link = base + iterative_part_numbers + generated_number\n",
    "    \n",
    "    if generated_link not in generated_links:\n",
    "        generated_links.append(generated_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 16 links, starting from the beginning of May were created, I also create 15K links goint to the past, to be sure that no links were missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate hexes for links\n",
    "# previous 15000 links\n",
    "\n",
    "selection = [hex(number).split('x')[-1] for number in range(0,256)]\n",
    "selection = ['0' + number.upper() if len(number) == 1 else number.upper() for number in selection]\n",
    "\n",
    "generated_links_prev = []\n",
    "\n",
    "base = 'https://dhs.riigikantselei.ee'\n",
    "iterative_part_numbers = '/avalikteave.nsf/documents/NT'\n",
    "\n",
    "for i in range(15000):\n",
    "    generated_number = '00' + str(hex(int('37A80A', 16) + i)).upper().split('X')[-1]\n",
    "    generated_link = base + iterative_part_numbers + generated_number\n",
    "    \n",
    "    if generated_link not in generated_links_prev:\n",
    "        generated_links_prev.append(generated_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will web scrape 16K links that were just automatically generated, parse all documents to predifined tabel and save results to MS Excel XLSX files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing scraping... 20:51:44\n",
      "Saving.. 11_06_2021_20_54_52_408_temp_.xlsx on 408 iteration 20:54:52\n",
      "Saving.. 11_06_2021_20_58_04_812_temp_.xlsx on 812 iteration 20:58:05\n",
      "Saving.. 11_06_2021_21_01_16_1212_temp_.xlsx on 1212 iteration 21:01:16\n",
      "Saving.. 11_06_2021_22_03_03_7720_temp_.xlsx on 7720 iteration 22:03:03\n",
      "Saving.. 11_06_2021_22_06_09_8128_temp_.xlsx on 8128 iteration 22:06:09\n",
      "Saving.. 11_06_2021_22_09_12_8532_temp_.xlsx on 8532 iteration 22:09:12\n",
      "Saving.. 11_06_2021_22_12_12_8932_temp_.xlsx on 8932 iteration 22:12:12\n",
      "Saving.. 11_06_2021_22_15_15_9336_temp_.xlsx on 9336 iteration 22:15:15\n",
      "Saving.. 11_06_2021_23_14_19_15836_temp_.xlsx on 15836 iteration 23:14:19\n",
      "Saving.. 11_06_2021_23_17_19_16240_temp_.xlsx on 16240 iteration 23:17:19\n",
      "Saving.. 11_06_2021_23_18_08_16341_FINAL_.xlsx on 16341 iteration\n",
      "Scraping completed with 16341 runs altogether 1025 collected 23:18:09\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "collected_entries = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "iterator = 0\n",
    "\n",
    "print('Commencing scraping...', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "for l in generated_links:\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        contents = get_web_contents(l)\n",
    "        df = parse_entry(contents, l, iterator)\n",
    "        \n",
    "        if counter == 0:\n",
    "            collected_entries = df\n",
    "        else:\n",
    "            collected_entries = collected_entries.append(df)\n",
    "        \n",
    "        iterator += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        time.sleep(.5)\n",
    "        pass\n",
    "    \n",
    "    if iterator == 100:\n",
    "        now = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        filename = now + '_' + str(counter) + '_temp_.xlsx'\n",
    "        collected_entries.to_excel(filename)\n",
    "        print('Saving..', filename, 'on', str(counter), 'iteration', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        iterator = 0\n",
    "        time.sleep(5)\n",
    "        \n",
    "    counter += 1\n",
    "    \n",
    "now = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "filename = now + '_' + str(counter) + '_FINAL_.xlsx'\n",
    "collected_entries.to_excel(filename)\n",
    "print('Saving..', filename, 'on', str(counter), 'iteration')\n",
    "print('Scraping completed with', str(counter), 'runs altogether', str(len(collected_entries)), 'collected', datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will additionally web scrape 15K links that were just automatically generated, parse all documents to predifined tabel and save results to MS Excel XLSX files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing scraping... 09:37:02\n",
      "Saving.. 12_06_2021_10_00_30_2636_temp_.xlsx on 2636 iteration 10:00:30\n",
      "Saving.. 12_06_2021_10_03_37_3040_temp_.xlsx on 3040 iteration 10:03:37\n",
      "Saving.. 12_06_2021_10_06_43_3444_temp_.xlsx on 3444 iteration 10:06:43\n",
      "Saving.. 12_06_2021_10_09_49_3848_temp_.xlsx on 3848 iteration 10:09:49\n",
      "Saving.. 12_06_2021_10_12_51_4248_temp_.xlsx on 4248 iteration 10:12:51\n",
      "Saving.. 12_06_2021_10_53_14_8712_temp_.xlsx on 8712 iteration 10:53:15\n",
      "Saving.. 12_06_2021_10_56_17_9112_temp_.xlsx on 9112 iteration 10:56:17\n",
      "Saving.. 12_06_2021_10_59_19_9512_temp_.xlsx on 9512 iteration 10:59:20\n",
      "Saving.. 12_06_2021_11_02_33_9920_temp_.xlsx on 9920 iteration 11:02:34\n",
      "Saving.. 12_06_2021_11_05_38_10320_temp_.xlsx on 10320 iteration 11:05:39\n",
      "Saving.. 12_06_2021_11_46_05_14792_temp_.xlsx on 14792 iteration 11:46:05\n",
      "Saving.. 12_06_2021_11_47_45_15000_FINAL_.xlsx on 15000 iteration\n",
      "Scraping completed with 15000 runs altogether 1151 collected 11:47:45\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "collected_entries = pd.DataFrame()\n",
    "\n",
    "counter = 0\n",
    "iterator = 0\n",
    "\n",
    "print('Commencing scraping...', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "for l in generated_links_prev:\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        contents = get_web_contents(l)\n",
    "        df = parse_entry(contents, l, iterator)\n",
    "        \n",
    "        if counter == 0:\n",
    "            collected_entries = df\n",
    "        else:\n",
    "            collected_entries = collected_entries.append(df)\n",
    "        \n",
    "        iterator += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        time.sleep(.5)\n",
    "        pass\n",
    "    \n",
    "    if iterator == 100:\n",
    "        now = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "        filename = now + '_' + str(counter) + '_temp_.xlsx'\n",
    "        collected_entries.to_excel(filename)\n",
    "        print('Saving..', filename, 'on', str(counter), 'iteration', datetime.now().strftime(\"%H:%M:%S\"))\n",
    "        iterator = 0\n",
    "        time.sleep(10)\n",
    "        \n",
    "    counter += 1\n",
    "    \n",
    "now = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n",
    "filename = now + '_' + str(counter) + '_FINAL_.xlsx'\n",
    "collected_entries.to_excel(filename)\n",
    "print('Saving..', filename, 'on', str(counter), 'iteration')\n",
    "print('Scraping completed with', str(counter), 'runs altogether', \n",
    "      str(len(collected_entries)), 'collected', datetime.now().strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Check results\n",
    "I wil check results of downloaded files to see if a nice table format was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Kellelt</th>\n",
       "      <th>Kellele</th>\n",
       "      <th>Väljaandja</th>\n",
       "      <th>Dok No</th>\n",
       "      <th>Kuupäev</th>\n",
       "      <th>Dok Tüüp</th>\n",
       "      <th>Dok Klass</th>\n",
       "      <th>AK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>https://dhs.riigikantselei.ee/avalikteave.nsf/...</td>\n",
       "      <td></td>\n",
       "      <td>Kandidaat</td>\n",
       "      <td></td>\n",
       "      <td>21-00376-10</td>\n",
       "      <td>07.05.2021</td>\n",
       "      <td>Kiri</td>\n",
       "      <td>18 Avaliku teenistuse tippjuhtide värbamine ja...</td>\n",
       "      <td>Asutusesiseseks kasutamiseks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>https://dhs.riigikantselei.ee/avalikteave.nsf/...</td>\n",
       "      <td>Justiitsministeerium</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>21-01098-1</td>\n",
       "      <td>07.05.2021</td>\n",
       "      <td>Määruse eelnõu</td>\n",
       "      <td>02 Vabariigi Valitsuse istungite ja nõupidamis...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>https://dhs.riigikantselei.ee/avalikteave.nsf/...</td>\n",
       "      <td>Lääne-Viru Omavalitsuste Liit</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>21-01094-1</td>\n",
       "      <td>06.05.2021</td>\n",
       "      <td>Kiri</td>\n",
       "      <td>07 Vabariigi Valitsuse ja peaministri muu asja...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  \\\n",
       "200  https://dhs.riigikantselei.ee/avalikteave.nsf/...   \n",
       "201  https://dhs.riigikantselei.ee/avalikteave.nsf/...   \n",
       "202  https://dhs.riigikantselei.ee/avalikteave.nsf/...   \n",
       "\n",
       "                           Kellelt    Kellele Väljaandja       Dok No  \\\n",
       "200                                 Kandidaat             21-00376-10   \n",
       "201           Justiitsministeerium                         21-01098-1   \n",
       "202  Lääne-Viru Omavalitsuste Liit                         21-01094-1   \n",
       "\n",
       "        Kuupäev        Dok Tüüp  \\\n",
       "200  07.05.2021            Kiri   \n",
       "201  07.05.2021  Määruse eelnõu   \n",
       "202  06.05.2021            Kiri   \n",
       "\n",
       "                                             Dok Klass  \\\n",
       "200  18 Avaliku teenistuse tippjuhtide värbamine ja...   \n",
       "201  02 Vabariigi Valitsuse istungite ja nõupidamis...   \n",
       "202  07 Vabariigi Valitsuse ja peaministri muu asja...   \n",
       "\n",
       "                               AK  \n",
       "200  Asutusesiseseks kasutamiseks  \n",
       "201                                \n",
       "202                                "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['URL','Kellelt','Kellele','Väljaandja','Dok No','Kuupäev','Dok Tüüp','Dok Klass','AK']\n",
    "pd.read_excel('11_06_2021_23_18_08_16341_FINAL_.xlsx')[columns][200:203].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
